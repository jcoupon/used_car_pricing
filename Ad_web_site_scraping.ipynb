{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ad web site scraping \n",
    "### Preamble\n",
    "\n",
    "Librairies and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from datetime import date, datetime, timedelta\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/root_link.txt', 'r') as filein:\n",
    "    root_link = filein.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_header = [\n",
    "    'item_id',\n",
    "    'link',\n",
    "    'description',\n",
    "    'name',\n",
    "    'price',\n",
    "    'km',\n",
    "    'year',\n",
    "    'engine',\n",
    "    'date',\n",
    "    'location',\n",
    "]\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\" Get page content using \n",
    "    Beautiful soup package.\n",
    "    \"\"\"\n",
    "    page = requests.get(url)\n",
    "    if page.status_code == 200:\n",
    "        # convert page content into a beautifulsoup object\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    else:\n",
    "        raise Exception('the page cannot be found')\n",
    "    \n",
    "    return soup\n",
    "\n",
    "def get_info(row, result_dict):\n",
    "    \"\"\"Get info for each row\"\"\"\n",
    "\n",
    "    item_id = int(re.search('\"id\":(\\d+)', data_tracking).group(1))\n",
    "    link = root_link+'/'+row.find('a').get('href')\n",
    "    description = row.find(\n",
    "        'div', {'class': 'listing-description'}).text.replace('\\t', '').replace('\\r', '')\n",
    "\n",
    "    try:\n",
    "\n",
    "        name = row.find('a').text.replace('\\t', '').replace('\\n', '').replace('\\r', '')\n",
    "        try:\n",
    "            price = float(\n",
    "                ''.join(re.findall('\\d+', row.find('div', {'class': 'listing-price'}).text)))\n",
    "        except:\n",
    "            price = None\n",
    "        km = re.search('\\d+', row.find_all('li')[0].text).group(0)\n",
    "        year = re.search('\\d+', row.find_all('li')[1].text).group(0)\n",
    "        engine = row.find_all('li')[2].text.split(':')[1].replace(' ', '')\n",
    "        date = row.find_all('li')[3].text\n",
    "        location = row.find_all('li')[4].text\n",
    "\n",
    "        result_dict['item_id'].append(item_id)\n",
    "        result_dict['link'].append(link)\n",
    "        result_dict['description'].append(description)\n",
    "        result_dict['name'].append(name)\n",
    "        result_dict['price'].append(price)\n",
    "        result_dict['km'].append(km)\n",
    "        result_dict['year'].append(year)\n",
    "        result_dict['engine'].append(engine)\n",
    "        result_dict['date'].append(date)\n",
    "        result_dict['location'].append(location)\n",
    "    \n",
    "    except:\n",
    "            \n",
    "        result_dict['item_id'].append(item_id)\n",
    "        result_dict['link'].append(link)\n",
    "        result_dict['description'].append(description)\n",
    "        result_dict['name'].append(None)\n",
    "        result_dict['price'].append(None)\n",
    "        result_dict['km'].append(None)\n",
    "        result_dict['year'].append(None)\n",
    "        result_dict['engine'].append(None)\n",
    "        result_dict['date'].append(None)\n",
    "        result_dict['location'].append(None)\n",
    "    \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date of scraping\n",
    "path_dir = '29.06.2019'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe guard\n",
    "proceed = False\n",
    "\n",
    "if proceed:\n",
    "    # create result dictionary\n",
    "    result_dict = {k: [] for k in table_header}\n",
    "\n",
    "    # the total number of pages is roughly 8300\n",
    "    for i in range(1, 8401):\n",
    "\n",
    "        # base url\n",
    "        url = root_link+'/fr/automobiles-voitures-de-tourisme--113/advertlist.aspx?pi={}'.format(i)\n",
    "        \n",
    "        # get page\n",
    "        soup = get_soup(url)\n",
    "\n",
    "        # loop over ads\n",
    "        rows = soup.find_all('li', {'class': 'list-item listing'})\n",
    "        for row in rows:\n",
    "            data_tracking = row.get('data-tracking')\n",
    "            get_info(row, result_dict)\n",
    "\n",
    "        # save file every 100 pages\n",
    "        # in case the loop fail and need \n",
    "        # to be rerun\n",
    "        if i%100 == 0:\n",
    "            print(i)\n",
    "            pd.DataFrame(result_dict).set_index('item_id').to_csv('{}/{}.csv'.format(path_dir, i))\n",
    "            result_dict = {k: [] for k in table_header}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regroup data into one single dataframe\n",
    "df_list = []\n",
    "for file in os.listdir(path_dir):\n",
    "    \n",
    "    if file[-3:] == 'csv':\n",
    "        df_list.append(pd.read_csv(path_dir+'/'+file, index_col='item_id'))\n",
    "        \n",
    "df = pd.concat(df_list)\n",
    "df = df.loc[df.index.drop_duplicates()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "Clean and transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = df['name'].str.split(expand=True)\n",
    "\n",
    "df['brand'] = name[0]\n",
    "df['model'] = name[1]\n",
    "df['model details'] = name[2]\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], dayfirst=True)\n",
    "df['year'] = pd.to_datetime(df['year'].fillna(0).astype(int), format=('%Y'), errors='coerce')\n",
    "\n",
    "df['age_car_years'] = (datetime.now()-df['year']).dt.days/365.\n",
    "df['age_ad_days'] = (datetime.now()-df['date']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check\n",
    "ax = df['age_ad_days'].hist()\n",
    "ax.figsize = (20,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path_dir+'_all.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
